---
title: Analisi sulla Produzione dei Rifiuti nel Friuli-Venezia Giulia
subtitle: Progetto dell'Esame di Statistica Computazionale e Modelli Multivariati
date: "A.A. 2024/2025"
author:
  - name: Erik De Luca
    email: erik.deluca@studenti.units.it
    affiliations:
      - name: Università degli Studi di Trieste
        city: Trieste
        state: Italia
abstract: |
  Si consideri l’insieme di dati contenuti nel file RU_FVG_22_semestre-2 utilizzando il linguaggio SAS dopo aver importato il file scrivendo gli eventuali problemi e indicando come sono stati risolti si scrivano i comandi necessari e si commentino i risultati per:
  
    a. Calcolare le statistiche univariate e le opportune rappresentazioni grafiche per le variabili e commentare i risultati.
    
    b. Calcolare le frequenze assolute e le tabelle a doppia entrata (si utilizzi l’opzione \\chisq) ricodificando preventivamente le variabili ove necessario e commentare i risultati.
    
    c. Calcolare la matrice di correlazione commentando i risultati.
    
    d. Nell’ipotesi in cui si ritenga necessario standardizzare le variabili (giustificare).
    
    e. Effettuare un’analisi dei cluster gerarchica (eventualmente su un campione di dati) utilizzando i diversi metodi (legame singolo, completo, etc…), individuare il metodo migliore spiegare perché lo si è scelto. Commentare i risultati indicando quali comuni appartengono ai cluster scelti.
    
    f. Dare come input della successiva cluster non gerarchica il numero di cluster ottenuti con la cluster gerarchica. Commentar l’output e i risultati. Confrontare i risultati delle due analisi dei clusters.
    
excecute:
  echo: false
  freeze: auto
lang: it
format:
  titlepage-pdf:
    titlepage: formal
    titlepage-logo: "../images/copertina.png"
    titlepage-header: "UNITS"
    titlepage-footer: "A.A. 2024/2025"
    titlepage-theme:
      elements: ["\\headerblock", "\\titleblock",  "\\logoblock", "\\authorblock", "\\footerblock"]
      page-align: "center"
      title-style: "doublelinetight"
      # title-fontstyle: ["huge", "bfseries"]
      title-space-after: "3.5cm"
      subtitle-fontstyle: "Large"
      author-style: "plain"
      affiliation-style: "none"
      author-fontstyle: "textsc"
      affiliation-fontstyle: "large"
      logo-space-after: "11\\baselineskip"
      logo-size: "0.9\\textwidth"
      header-fontstyle: ["textsc", "LARGE"]
      header-space-after: "1.5cm"
  # pdf:
    # pdf-engine: latexmk
    # header-includes: |
    #   \usepackage{titling}
    #   \pretitle{\begin{center}\includegraphics[width=6in]{../images/copertina.png}\end{center}\vspace{1em}}
    toc: true
    number-sections: true
    colorlinks: true
    documentclass: report
    df-print: kable
    fig-width: 6
    fig-height: 4
    geometry:
      - top=20mm
      - left=25mm
      - right=25mm
      # - bottom = 20mm
      - heightrounded
editor: 
  markdown: 
    wrap: sentence
output-file: Analisi dei Rifiuti nei Comuni del FVG
---

\newpage

## Consegna {.unnumbered .unlisted .hidden}

Si consideri l’insieme di dati contenuti nel file RU_FVG_22_semestre-2 utilizzando il linguaggio SAS dopo aver importato il file scrivendo gli eventuali problemi e indicando come sono stati risolti si scrivano i comandi necessari e si commentino i risultati per:

a.  Calcolare le statistiche univariate e le opportune rappresentazioni grafiche per le variabili e commentare i risultati.

b.  Calcolare le frequenze assolute e le tabelle a doppia entrata (si utilizzi l’opzione \\chisq) ricodificando preventivamente le variabili ove necessario e commentare i risultati.

c.  Calcolare la matrice di correlazione commentando i risultati.

d.  Nell’ipotesi in cui si ritenga necessario standardizzare le variabili (giustificare).

e.  Effettuare un’analisi dei cluster gerarchica (eventualmente su un campione di dati) utilizzando i diversi metodi (legame singolo, completo, etc…), individuare il metodo migliore spiegare perché lo si è scelto.
    Commentare i risultati indicando quali comuni appartengono ai cluster scelti.

f.  Dare come input della successiva cluster non gerarchica il numero di cluster ottenuti con la cluster gerarchica.
    Commentar l’output e i risultati.
    Confrontare i risultati delle due analisi dei clusters.

# Preparazione dati in R

## Importazione librerie e dati

I dati, contenuti in un foglio Excel, sono stati dapprima manipolati in Excel e successivamente in R.
Grazie al pacchetto `{tidyverse}` si è gestito tutto il processo riguardante il preprocessamento dati.
Ovviamente, ciò sarebbe stato possibile anche attraverso il software SAS.

Dal foglio iniziale in Excel sono state estratte le informazioni essenziali per l'analisi e divise in 3 diversi fogli per una gestione separata dei dati.
Il primo foglio contiene i dati del comune, il secondo contiene l'analisi dettagliata dei rifiuti e la terza raccoglie degli indicatori basati sui dati del primo e secondo foglio.
L'output seguente è un esempio del contenuto del foglio Excel sugli indicatori.

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(readxl)
library(visdat)

# Importazione dati da fogli excel che iniziano con R_
info <- read_excel(
  here::here("data/RU_FVG_22_semestre-2.xlsx"),
  sheet = "R_info"
  )

rifiuti <- read_excel(
  here::here("data/RU_FVG_22_semestre-2.xlsx"),
  sheet = "R_rifiuti"
  )

indicatori <- read_excel(
  here::here("data/RU_FVG_22_semestre-2.xlsx"),
  sheet = "R_indicatori"
  )

print_df <- function(data, nrow = 5, ncol = 5, accuracy = .01)
{
  decimals <- -log10(accuracy)
  data |> 
    slice_head(n = nrow) |> 
    select(1:ncol) |> 
    mutate(across(where(is.numeric), \(x) round(x, decimals))) 
}

print_df(indicatori)
```

## Rinominazione colonne

Osservando i nomi delle colonne ci sono casi di omonimia.

```{r}
rifiuti |> 
  names() |> 
  tibble(rifiuti = _) |> 
  filter(if_any(rifiuti, \(x) str_detect(x, "\\.\\.\\."))) |>
  slice_sample(n = 5) |> 
  bind_cols(indicatori |> 
              names() |> 
              tibble(indicatori = _) |> 
              filter(if_any(indicatori, \(x) str_detect(x, "\\("))) |> 
              slice_sample(n = 5)) 
```

Unisco le colonne con lo stesso nome e faccio una somma dei valori.
Inoltre per le colonne di indicatori elimino il contenuto tra parentesi per una lettura più semplice.

```{r}
# elimino le parentesi e il loro contenuto
indicatori |> 
  rename_with(~str_remove(.x, "[:space:]\\(.*\\)"), -1) -> indicatori

# raggruppo colonne con lo stesso nome effettuando la somma
rifiuti |> 
  pivot_longer(cols = -Istat, names_to = "group", values_to = "value") |> 
  mutate(group = str_remove(group, "\\.\\.\\..*")) |> 
  pivot_wider(
    names_from = group,
    values_from = value,
    values_fn = \(x) sum(x, na.rm = T)
    ) |> 
  # mutate 0 in NA
  mutate(across(where(is.numeric), ~na_if(.x, 0))) -> rifiuti
```

## Dati mancanti

Come si può vedere ci sono alcune colonne con un alto tasso di dati mancanti e alcune righe con molti dati mancanti.

Potrei valutare di eliminare i comuni che raccolgono pochi dati ed eliminare i tipi di rifiuti che i coumni normalemente non raccolgono o non classificano.

```{r}
#| fig-height: 5
rifiuti |> 
  summarise(across(everything(), ~sum(is.na(.x))/n())) |> 
  pivot_longer(cols = everything(), names_to = "col", values_to = "miss") |>
  mutate(col = fct_reorder(col, miss, .desc = F,
                           .na_rm = F, .fun = ~ sum(., na.rm = T))) |> 
  ggplot(aes(miss, col, label = scales::percent(miss, accuracy = .1))) +
  geom_col(fill = "tomato", alpha = .9) +
  geom_text(hjust = "inward", size = 3) +
  scale_x_continuous(labels = scales::percent) +
  theme_minimal() +
  labs(
    x = "",
    y = "",
    title = "Numero di osservazioni mancanti per colonna"
  ) +
  theme(plot.title.position = "plot")
```

Elimino le colonne con oltre l'80% di dati mancanti.
Infatti queste colonne sarebbero difficili da trattare.
In un'analisi più approfondita potrebbero essere considerate come: cura di dettaglio della raccolta dati che a sua volta può essere indice di una migliore raccolta differenziata da parte del comune.

```{r}
rifiuti |> 
  summarise(across(everything(), ~sum(is.na(.x))/n())) |> 
  pivot_longer(cols = everything(), names_to = "col", values_to = "miss") |>
  filter(miss > 0.8) |>
  pull(col) -> drop_cols

rifiuti |>
  select(-all_of(drop_cols)) -> rifiuti

print_df(tibble("Colonne Eliminate" = drop_cols), ncol = 1)
```

I comuni con oltre il 50% di dati mancanti sono 42.
Le opzioni sono due:

1.  Eliminare i comuni con molti dati mancanti

2.  Imputare i dati tramite media o mediana, o tramite un modello di regressione che stima i dati mancanti considerando la provincia e la popolazione

```{r}
rifiuti |>
  rowwise() |>
  transmute(
    Istat,
    miss = round(sum(is.na(c_across(where(is.numeric)))) / ncol(rifiuti), 2)
    ) |> 
  ungroup() |> 
  filter(miss > 0.5) |> 
  arrange(desc(miss)) |>
  left_join(info, by = "Istat") |> 
  print_df(nrow = 10)
```

### Imputazione dati mancanti

Nel grafico viene rappresentato un esempio di imputazione dei dati mancanti tramite regressione lineare per i rifiuti:

-   Verde

-   Metalli

-   Plastica

-   Organico

Un modello più complesso potrebbe essere considerare le variabili disponibili sugli altri rifiuti e sugli indicatori ma per semplicità considero solo la popolazione e la provincia.

```{r, warning=FALSE, message=FALSE}
#| fig-height: 5
rifiuti |> 
  left_join(info, by = "Istat") |>
  pivot_longer(
    cols = -c(Istat, Provincia, Popolazione, Comune),
    names_to = "rifiuto",
    values_to = "peso"
    ) |> 
  filter(rifiuto %in% c("Verde", "Plastica", "Metalli", "Organico")) |>
  # filter(is.na(peso))
  ggplot(aes(x = Popolazione, y = peso, color = Provincia)) +
  geom_point(alpha = .7, size = .9) +
  facet_wrap(~rifiuto, ncol = 2, scales = "free_y") +
  geom_smooth(method = "lm", se = F) +
  scale_x_log10(
    labels = scales::label_number_si(accuracy = 1)
  ) +
  scale_y_log10(
    labels = scales::label_number_si(accuracy = 1, suffix = " Kg")
  ) +
  labs(title = "Dimensione Rifiuti per Provincia e Popolazione",
       x = "Popolazione",
       y = "Verde") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

Per semplificare l'imputazione creo una funzione che crea un modello per ciascuna variabile contenente NAs e procede a riempire il dataset con i nuovi dati generati

```{r}
# Imputazione attraverso regressione lineare
imputa_dati <- function(data, var_mancante, var_predictors) {
  # Rimuoviamo i casi NA per il fitting del modello
  data_non_na <- data |> filter(!is.na(!!sym(var_mancante)))
  
  # Adattiamo il modello di regressione
  formula <- as.formula(paste0("`", var_mancante, "` ~ ", 
                               paste(var_predictors, collapse = " + ")))
  modello <- lm(formula, data = data_non_na)
  
  # Prevediamo i valori mancanti
  data |>
    mutate(
      !!sym(var_mancante) := ifelse(is.na(!!sym(var_mancante)),
                                    predict(modello, newdata = data) |> round(0),
                                    !!sym(var_mancante))
    ) -> data
  return(data)
}

reduce(
  colnames(rifiuti)[colSums(is.na(rifiuti)) > 0],
  ~imputa_dati(.x, .y, c("Provincia", "Popolazione")),
  .init = rifiuti |> left_join(info, by = "Istat")
) |>
  select(-c(Popolazione, Provincia, Comune)) -> rifiuti

```

## Trasformazione Variabili

Alcuni indici sono per persona mentre altri sono globali.
Per una migliore analisi li trasformo tutti per persona (pro capite).

```{r}
indicatori |> 
  left_join(
    info |> select(Istat, Popolazione),
    by = "Istat"
  ) |> 
  mutate(
    across(c(Differenziati, Indifferenziati), \(x) x / Popolazione * 1E3)
  ) |> 
  select(-Popolazione, -`Totale RU`) -> indicatori

indicatori |> 
  print_df(ncol = 7)
```

## Aggiunta Variabili Categoriche

Per la consegna di un esercizio bisogna creare una tabella a doppia entrata.
Per far ciò devo usare due variabili qualitative, le otterrò categorizzando le variabili numeriche degli indicatori.
Creo, quindi, un nuovo dataset chiamato *indicatori_categorici* che conterrà tutti gli indicatori in forma di variabili qualitative con 3 categorie:

1.  Basso

2.  Medio

3.  Alto

L'assegnazione della categoria è basata sui quantili della variabile.

```{r}
# dividi in 3 categorie
indicatori |> 
  mutate(
    across(
      where(is.numeric),
            ~ cut(
              .x,
              breaks = quantile(.x,
                                probs = seq(0, 1, by = 1/3), # c(0.33, .66, 1)
                                na.rm = TRUE),
              labels = c("Basso", "Medio", "Alto"),
              include.lowest = TRUE)
    )
  ) -> indicatori_categorici
```

## Esportazione dati

Ora che i dati sono puliti e pronti li *impacchetto* con dei join e li salvo in formato *csv*, un formato leggero e facilmente importabile.

Nel creare un unico file con tutti i dati uniti, devo rinominare gli indicatori per evitare casi di omonimia (e.g. Vetro).

```{r}
# solo rifiuti
info |> 
  right_join(rifiuti, by = "Istat") |>
  write_csv(here::here("data/rifiuti.csv"))

# solo indicatori
info |> 
  right_join(indicatori, by = "Istat") |> 
  write_csv(here::here("data/indicatori.csv"))

# indicatori categorici
info |> 
  right_join(indicatori_categorici, by = "Istat") |> 
  write_csv(here::here("data/indicatori_categorici.csv"))

# rifiuti e indicatori
info |> 
  right_join(rifiuti, by = "Istat") |>
  # rename indicatori con I_ per evitare conflitti
  right_join(indicatori |> rename_with(~str_c("I_", .x), -1), by = "Istat") |> 
  write_csv(here::here("data/rifiuti_indicatori.csv"))
```

# Analisi dati in SAS

## Importazione dei dati

Carico in SAS i diversi file *csv* e li salvo nelle rispettive nuove variabili.

-   `datafile = "/home/u64115021/sasuser.v94/.*"`: Percorso della posizione del file.

-   `out`: Nome della variabile salvata in SAS.

-   `dbms=csv` (*Data Base Management System*): Serve a indicare il tipo di formato dei dati.
    In questo caso in formato csv.

-   `replace`: Se la variabile è già presente, allora andrà sostituita.

-   `getnames=yes`: Usa la prima riga come nomi delle variabili del dataset.

```{sas, eval=FALSE}
proc import datafile="/home/u64115021/sasuser.v94/rifiuti.csv"
    out=rifiuti
    dbms=csv
    replace;
    getnames=yes; 
run;

proc import datafile="/home/u64115021/sasuser.v94/indicatori.csv"
    out=indicatori
    dbms=csv
    replace;
    getnames=yes; 
run;

proc import datafile="/home/u64115021/sasuser.v94/rifiuti_indicatori.csv"
    out=rifiuti_indicatori
    dbms=csv
    replace;
    getnames=yes; 
run;

proc import datafile="/home/u64115021/sasuser.v94/indicatori_categorici.csv"
    out=indicatori_categorici
    dbms=csv
    replace;
    getnames=yes; 
run;
```

## Statistiche univariate e rappresentazioni grafiche

Calcolo delle statistiche univariate e rappresentazione grafica.
Per semplicità l'analisi è stata eseguita solo sulla variabili *RU* (rifiuto urbano) presente nel dataset *indicatori*.

-   `proc univariate`: Calcola le statistiche univariate per un singolo indice del dataset.

-   `data=indicatori`: Specifica il dataset su cui calcolare le statistiche.

-   `var RU`: Indica la variabile su cui calcolare le statistiche, in questo caso *RU*.

```{sas, eval=FALSE}
/* Calcolare statistiche univariate solo su un indice per semplicità */
proc univariate data=indicatori;
    var RU; 
run;
```

\begin{figure}
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/momenti.png}
        \caption{Momenti}
        \label{fig:momenti}
        \vspace{0.5cm} 
        \includegraphics[width=\textwidth]{../images/statistiche_di_base.png}
        \caption{Statistiche di base}
        \label{fig:stat-base}
    \end{subfigure}\hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[height=0.9\textheight]{../images/quantili.png}
        \caption{Quantili}
        \label{fig:quantili}
    \end{subfigure}
    \caption{Variabile RU (rifiuto urbano)}
    \label{fig:stat-base-ru}
\end{figure}

<!-- ::: {#fig-stat-base layout-ncol=2} -->

<!-- ![Momenti](../images/momenti.png){#fig-momenti} -->

<!-- ![Statistiche di base](../images/stattistiche_di_base.png){#fig-stat} -->

<!-- ![Quantili](../images/quantili.png){#fig-quantili} -->

<!-- Alcune statistiche della variabile RU -->

<!-- ::: -->

Produco un istogramma per vedere come si distribuisce la variabile RU.

-   `proc sgplot`: Crea rappresentazioni grafiche del dataset.

-   `histogram RU`: Genera un istogramma per la variabile *RU*.

```{sas, eval=FALSE}
/* Rappresentazioni grafiche */
proc sgplot data=indicatori;
    histogram 'Totale RU'n;
run;
```

![Istogramma dei rifiuti urbani per capita nei comuni della regione FVG](../images/istogramma.png){fig-alt="Istogramma del totale di rifiuti urbani nei comuni della regione FVG" width="75%"}

```{sas, eval=FALSE}
proc sgplot data=indicatori;
    scatter y=Differenziati x=Carta;
    yaxis label="Differenziati";
    xaxis label="Carta";
run;
```

Osservando l'istogramma si nota che i dati potrebbero essere approssimati da una normale.
C'è la presenza di un principale outlier: Lignano Sabbiadoro.
Lignano, come gli altri comuni con un indice di rifiuti urbani per capita maggiore, sono località turistiche.
La popolazione residente è bassa ma c'è un grande afflusso di gente e di produzione di rifiuti durante le stagioni turistiche.

```{r}
indicatori |> 
  left_join(info, by = "Istat") |> 
  select(Comune, Popolazione, RU) |> 
  arrange(-RU) |> 
  print_df(ncol = 3, nrow = 5)
```

Nello scatterplot, ogni punto rappresenta un comune, illustrando la relazione tra i rifiuti differenziati e carta.
La distribuzione dei punti suggerisce una correlazione positiva.

![Scatterplot](../images/scatter_plot.png){width="75%"}

## Tabelle a singola e doppia entrata

-   `proc freq`: Calcola le frequenze delle variabili nel dataset.

-   `data=indicatori_categorici`: Specifica il dataset che contiene le variabili d’interesse.

-   `tables Provincia / nocum`: Crea una tabella di frequenze assolute per la variabile ‘Provincia’ senza visualizzare i totali cumulativi in quanto non è una variabile categorica ordinata.

```{sas, eval=FALSE}
/* Frequenze assolute */
proc freq data=indicatori_categorici;
    tables Provincia / nocum;
run;
```

![Numero di comuni per provincia](../images/freq_univariate.png){fig-alt="Numero di comuni per provincia" width="30%"}

-   `proc freq`: Utilizzato ancora per calcolare le frequenze, ma questa volta per due variabili.

-   `tables RD*Provincia / chisq`: Crea una tabella a doppia entrata per le variabili *RD* (rifiuti differenziati) e *Provincia*, includendo il test chi-quadrato per valutare l’associazione tra le due variabili.

```{sas, eval=FALSE}
/* Tabelle a doppia entrata*/
proc freq data=indicatori_categorici;
    tables RD*Provincia / chisq;
run;
```

![Tabella di frequenze a doppia entrata](../images/freq_bivariate.png){fig-alt="Tabella di frequenze a doppia entrata con ulteriori statistiche" width="50%"}

Il valore della statistica Chi-quadro suggerisce che c'è dipendenza tra la percentuale di rifiuti che vengono differenziati e la provincia d'appartenenza del comune in questione.
Infatti i comuni delle provincie di Gorizia e Pordenone tendono a fare maggior raccolta differenziata, i comuni della provincia di Trieste sarebbero, invece, i meno virtuosi.

![Statistiche d'indipendenza della variabile RD rispetto a Provincia](../images/chi-quadro.png){fig-alt="Tabella di frequenze a doppia entrata con ulteriori statistiche" width="50%"}

## Matrice di Correlazione

Calcolo della matrice di correlazione per tutte le variabili numeriche nel dataset indicatori, escludendo ‘istat’ trasformato in carattere.
Ecco come:

Trasformazione della variabile *istat*:

-   `data indicatori`: Crea un nuovo dataset con lo stesso nome per elaborare i dati.

-   `set indicatori`: Importa i dati originali.

-   `istat_char = put(istat, 8.)`: Converte la variabile *istat* in formato carattere.

-   `drop istat`: Elimina la variabile originale *istat*.

-   `rename istat_char = istat`: Rinomina la variabile convertita per mantenere la coerenza nel dataset.

```{sas, eval=FALSE}
/* Matrice di correlazione */
/* Prima trasformo la variabile Istat in carattere in modo da non considerarla
nell'analisi*/
data indicatori;
    set indicatori;
    istat_char = put(istat, 8.); /* Conversione in carattere */
    drop istat; /* Rimuovo l'originale */
    rename istat_char = istat;
run;
```

Calcolo della matrice di correlazione:

-   `proc corr`: Calcola la matrice di correlazione per le variabili numeriche.

-   `var _numeric_`: Specifica che l’analisi deve essere effettuata su tutte le variabili numeriche del dataset.

```{sas, eval=FALSE}
proc corr data=indicatori;
    var _numeric_; 
run;
```

![Matrice di correlazione](../images/matrice_correlazione.png)

## Clustering Gerarchico

Effettuiamo un’analisi di clustering gerarchico sui 20 comuni più popolosi utilizzando diversi metodi.

Selezione dei comuni più popolosi:

-   `proc sql outobs=20;`: Utilizza SQL per selezionare i primi 20 comuni per popolazione, `outbus` sostituisce il comando `LIMIT` in SAS.

-   `create table top20_comuni as`: Crea una nuova tabella con questi comuni.

-   `order by Popolazione desc;`: Ordina i comuni in base alla popolazione, in ordine decrescente.

```{sas, eval=FALSE}
/* Eseguo l'analisi solo sui 20 comuni più popolosi */
/* In SAS non c'è il comando LIMIT per SQL, quindi bisogna usare outobs*/
proc sql outobs=20; 
   create table top20_comuni as
   select *
   from rifiuti_indicatori
   order by Popolazione desc;
quit;
```

Standardizzazione delle variabili:

-   `proc standard`: Standardizza le variabili numeriche per avere media 0 e deviazione standard 1.

```{sas, eval=FALSE}
/* Standardizza le variabili */
proc standard data=top20_comuni  
	mean=0 
	std=1 
	out=top20_comuni_std;
  var _NUMERIC_; 
run;
```

:::{.callout-note}

In SAS, a differenza di R, non è necessario calcolare le distanze prima di applicare il clustering gerarchico. In questo progetto, per i metodi utilizzati, verrà sempre impiegata la distanza Euclidea. 

:::

### Metodo del legame singolo

Unisce i cluster sulla base della distanza minima tra i punti di due cluster.
Può creare cluster allungati e meno compatti.
Infatti, osservando il dendogramma, si nota che a ogni iterazione dell'algoritmo viene creato un nuovo cluster quasi sempre solo con un comune.

```{sas, eval=FALSE}
/* Metodo del legame singolo*/
proc cluster data=top20_comuni_std method=single outtree=tree_single;
   var _numeric_; /* Sostituisci con le tue variabili */
   id Comune;
run;
```

![Dendogramma del cluster gerarchico con il metodo del legame singolo](../images/legame_singolo.png){width="60%"}

### Metodo del legame completo

Unisce i cluster considerando la distanza massima tra i punti di due cluster.

```{sas, eval=FALSE}
/* Metodo del legame completo*/
proc cluster data=top20_comuni_std method=complete outtree=tree_complete;
   var _numeric_;
   id Comune;
run;
```

![Dendogramma del cluster gerarchico con il metodo del legame completo](../images/legame_completo.png){fig-alt="Dendogramma del cluster gerarchico con il metodo del legame completo" width="60%"}

### Metodo del legame medio

Utilizza la distanza media tra tutti i punti di due cluster per unirli, bilanciando tra legame singolo e completo.

```{sas, eval=FALSE}
/* Metodo del legame medio*/
proc cluster data=top20_comuni_std method=average outtree=tree_average;
   var _numeric_;
   id Comune;
run;
```

![Dendogramma del cluster gerarchico con il metodo del legame medio](../images/legame_medio.png){fig-alt="Dendogramma del cluster gerarchico con il metodo del legame medio" width="60%"}

### Metodo di Ward

Minimizza la varianza totale all’interno dei cluster.
Produce cluster di dimensioni simili e ben separati.

-   `ccc`: Il Cubic Clustering Criterion (CCC) fornisce una misura per valutare l’adeguatezza del numero di cluster scelto. Rimossa in quanto dava valori nulli dal quarto cluster in poi.

-   `pseudo`: L’opzione pseudo genera statistiche pseudo-$F$ e pseudo-$t^2$, che sono utili per identificare i cluster significativi e valutare la qualità della suddivisione dei dati.

```{sas, eval=FALSE}
/* Metodo di Ward*/
proc cluster data=top20_comuni_std
  method=ward
  outtree=tree_ward 
  /* ccc */ 
  pseudo;
  var _NUMERIC_;
  id Comune;  
run;
```


La statistica pseudo-$t^2$ suggerisce un numero di cluster ottimale di quattro. Tuttavia, due dei quattro cluster sarebbero composti solo da una singola unità. Quindi, ad eccezione dei due outliers si otterrebbero solo due cluster con più unità. Per un analisi più rappresentativa risulta più opportuno scegliere un numero di cluster maggiore. Non si cerca più un massimo globale bensì un massimo locale, in questo caso il massimo locale scelto corrisponde a un numero di sei cluster. 

:::{.callout-note}

Si sarebbe potuto anche scegliere il cluster di tredici che con il valore della statistica pseudo-$t^2$ maggiore al cluster di sei. In questo caso però si avrebbe affrontato il problema opposto dei quattro cluster. Infatti, in questo caso la maggior parte di cluster sarebbero stati composti da una o due unità.

:::

\begin{figure}
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{../images/autovalori.png}
        \caption{Primi 10 autovalori}
        \label{fig:autovalori}
        \vspace{0.5cm} 
        \includegraphics[width=\textwidth]{../images/cronologia_cluster.png}
        \caption{Cronologia dei cluster}
        \label{fig:cronologia-cluster}
    \end{subfigure}\hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../images/ward_numero_cluster.png}
        \caption{Statistiche utili all'identificazione del numero di cluster}
        \label{fig:num-cluster}
        \vspace{0.5cm} 
        \includegraphics[width=\textwidth]{../images/ward.png}
        \caption{Dendogramma del cluster gerarchico}
        \label{fig:dendogramma}
    \end{subfigure}
    \caption{Cluster gerarchica usando il metodo di Ward}
    \label{fig:ward}
\end{figure}

<!-- ![Primi 10 autovalori ottenuti durante la cluster gerarchica usando il metodo di Ward](../images/autovalori.png){width="40%"} -->

<!-- ![Cronologia dei cluster ottenuti attraverso il metodo di Ward](../images/cronologia_cluster.png){width="60%"} -->

<!-- ![Statistiche utili all'identificazione del numero di cluster](../images/ward_numero_cluster.png){width="60%"} -->

<!-- ![Dendogramma del cluster gerarchico con il metodo di Ward](../images/ward.png){fig-alt="Dendogramma del cluster gerarchico con il metodo di Ward" width="60%"} -->


## Cluster Non Gerarchica

Nel caso della cluster non gerarchica il numero di cluster è fissato a priori, nel nostro caso 6.
La cluster gerarchica è utile nei casi di dataset elevati dove l'obiettivo è quello di evidenziare le caratteristiche non di singole unità ma dei gruppi.
Per questo motivo sono state prodotte due cluster non gerarchiche: una utilizzando gli stessi dati usati per le altre cluster, e un'altra con i dati di tutti i comuni del FVG.

### 20 comuni più popolosi

<!-- Il parametro `maxiter=100` imposta il numero massimo di iterazioni per trovare i centri dei cluster, mentre `converge=0.001` stabilisce il criterio di convergenza per interrompere l’iterazione quando i cambiamenti sono minimi. -->

-   `maxclusters=6`: fissa il numero massimo di cluster a sei, scelta basasta sulla cluster gerarchica del capitolo precedente.

-   `maxiter=100`: definisce il numero massimo di iterazioni per l’algoritmo, garantendo che il processo di clustering giunga a una conclusione stabile entro un numero ragionevole di passi.

-   `converge=0.001`: stabilisce la soglia di convergenza, con l’algoritmo che termina se il miglioramento tra iterazioni successive è inferiore a 0.001. Nonostante ci sia già un parametro che definisca l'uscita dal ciclo (`maxiter`), quest'altro garantisce una procedura più veloce, con meno passi, nei casi in cui la soglia di convergenza sia raggiunta.

-   `replace=full`: indica che i cluster verranno ricalcolati completamente in ogni iterazione, fornendo una maggiore accuratezza nel posizionamento dei centroidi. Trattando pochi dati ci si può permettere una maggior accuratezza a discapito di un maggior tempo macchina.

-   `radius=0`: specifica che non vi è alcuna limitazione sul raggio dei cluster, permettendo ai cluster di adattarsi liberamente alla distribuzione dei dati.

```{sas, eval=FALSE}
proc fastclus data=top20_comuni_std
  out=clus_out
  maxclusters=6
  maxiter=100
  converge=0.001
  replace=full 
  radius=0;
  var _numeric_;
run;
```

![Semi iniziali assegnati dalla procedura FASTCLUS del software SAS](../images/fastclus.png)

![Cronologia delle iterazioni e del cambio dei semi nei cluster](../images/iterazioni.png){width="40%"}

![Statistiche per variabili](../images/fastclus_statistiche.png){width="50%"}

Osservando la tabella si nota come nel primo cluster ci siano i comuni che producono una quantità maggiore di rifiuti secchi, indifferenziati e plastici, a discapito di rifiuti differenziati, organici e vetro.
Nel cluster 6, invece, si potrebbe ipotizzare che sono stati raggruppati i comuni senza un centro di smaltimento dei rifiuti in quanto hanno dei valori molto bassi per legno, metallo e RAEE (rifiuti di apparecchi elettrici ed elettronici).

![Medie dei cluster](../images/media_cluster.png)

### Tutti i comuni

Nel caso in cui vengano inclusi tutti i comuni la procedura `FASTCLUS` produce risultati diversi.
Dal riepilogo dei cluster si può notare come ci sia solo un cluster contenente un unico comune e un altro cluster con 3 comuni.

Guardando le medie dei cluster si capisce suibito come il cluster con un unico comune contenga Lignano Sabbiadoro.
Infatti il rifiuto urbano totale è di 1005.
Il cluster con i 3 comuni potrebbe essere composto dai comuni meno efficienti nella differenzazione dei rifiuti (RD = 40%).

```{sas, eval=FALSE}
/* Rimuovo Istat e popolazione */
data cluster_indicatori;
	set indicatori;
	drop Istat Popolazione;
run;

/* su tutti i comuni*/
proc fastclus data=cluster_indicatori 
  out=clus_out 
  maxclusters=6 
  maxiter=100 
  converge=0.001
  replace=full 
  radius=0;
  var _numeric_;
run;
```

![Riepilogo dei cluster](../images/fastclus_indicatori_riepilogo.png){width="50%"}

![Medie dei cluster](../images/fastclus_indicatori_medie.png){width="80%"}

# Elaborazioni aggiuntive

## PCA

Per una visualizzazione migliore del clustering eseguito in SAS, si è deciso di rappresentare i cluster in un biplot, ovvero un grafico dove sugli assi cartesiani sono presenti le prime due componenti principali.
Considerando le prime due componenti principali, catturerò il 41,6% della varianza totale.

```{r, warning=FALSE, message=FALSE}
# creo il df con i 20 comuni più popolosi
info |> 
  # right_join(rifiuti, by = "Istat") |>
  # rename indicatori con I_ per evitare conflitti
  right_join(
    indicatori,
    # indicatori |> rename_with(~str_c("I_", .x), -1),
    by = "Istat"
    ) |> 
  arrange(-Popolazione) |> 
  slice_head(n = 20) |> 
  select(-Popolazione, -Istat) -> data4pca

# calcolo le componenti principali
data4pca |> 
  select(where(is.numeric)) |> 
  scale() |> 
  prcomp() -> pca

# stampo il grafico
tibble(
  sd = pca$sdev,
  componente = 1:length(pca$sdev)
) |> 
  mutate(perc = scales::percent(cumsum(sd) / sum(sd), accuracy = .1)) |> 
  ggplot(aes(componente, sd, label = perc)) +
  geom_point() +
  geom_line() +
  geom_text(
    nudge_x = .7,
    nudge_y = .05,
    size = 3
  ) +
  theme_minimal() +
  scale_x_continuous(
    # labels = scales::number_format(suffix = "°"),
    breaks = seq(1, length(pca$sdev), by = 1)
    ) +
  theme(
    panel.grid.minor.x = element_line(linetype = 0)
  ) +
  labs(
    x = "Componente principale",
    y = "Deviazione standard",
    title = "Scomposizione della varianza in componenti principali",
    subtitle = "Il testo nel grafico rappresenta la % di varianza cumulata"
  )
```

Di seguito un grafico intuitivo per comprendere meglio come le variabili sono state ruotate nelle prime 6 componenti principali.
Il correlogramma mostra la correlazione tra le variabili originali e le componenti principali.

Nella prima componente principale si può vedere come i rifiuti differenziati e le loro componenti (*e.g.* vetro, organico, ...) sono contrapposti al rifiuto secco e indifferenziato.

:::{.callout-tip}
## Dettagli di Implementazione

Viene usata la funzione `continuous_scale`, anziché le funzioni standard di `{ggplot2}` in quanto essa offre una maggior flessibilità e personalizzazione.
In questo caso ho potuto dar in pasto una palette con 5 colori personalizzati come input e assegnarli manualmente i valori in modo tale che il colore centrale, il giallo, corrisponda alla correlazione nulla.
Per far ciò sono stati simulati i valori da una distribuzione beta con parametri $\alpha$ e $\beta$.
I parametri sono stati selezionati manualmente, altrimenti per una maggior precisione si sarebbe potuto optare per il pacchetto `{fitdistrplus}` che permette di stimare i parametri di una distribuzione dato un campione in base a diversi metodi (*e.g.* MLE, MSE, ...).
:::

```{r, warning=FALSE, message=FALSE}
pca$rotation |>
  data.frame() |> 
  rownames_to_column("rifiuti") |> 
  pivot_longer(-rifiuti, names_to = "PC") |> 
  mutate(
    across(PC, \(x) str_extract(x, "[:digit:]+") |> as.numeric()),
    # Ordina i rifiuti per i valori della prima colonna
    combined_order = if_else(PC == 1, value, 0), 
    across(
      rifiuti,
      \(x) fct_reorder(x, combined_order, .fun = "mean")
    )
    ) |> 
  filter(PC < 6) |>
  ggplot(aes(PC, rifiuti, fill = value)) +
  geom_tile() +
  scale_x_continuous(
    breaks = 1:10
  ) +
  continuous_scale(
    aesthetics = "fill",
    scale_name = "palette mia",
    name = "Correlazione",
    palette = scales::gradient_n_pal(
      colours = c("tomato4", "tomato", "yellow", "springgreen", "springgreen4"),
      values = pbeta(seq(0, 1, by = .1), shape1 = 5, shape2 = 6.5)
      ),
    breaks = seq(1, -1, by = -.2)
  ) +
  theme(
    # legend.position = "bottom"
    panel.grid.major = element_line(linetype = 0),
    panel.grid.minor = element_line(linetype = 0),
  ) +
  theme_minimal() +
  labs(
    x = "Componente principale",
    y = "",
    title = "Rotazione variabili"
  )
  
```

Sono stati ottenuti i 6 cluster, in SAS, dal metodo di Ward.
I dati sono stati quindi esportati e importati in R.

```{sas, eval=F}
/* Esporto i dati per visualizzarli in R */
/* Primo passo: Taglio l'albero per ottenere 6 cluster */
proc tree data=tree_ward nclusters=6 out=clusters;
   id Comune;
run;

/* Secondo passo: Visualizzo i comuni e i loro cluster */
proc print data=clusters;
   var Comune cluster;
run;

/* Terzo passo: Esporto i dati */
proc export data=clusters
   outfile="/home/u64115021/sasuser.v94/cluster_top20comuni.csv"
   dbms=csv
   replace;
run;
```

Infine, inserisco i comuni nel grafico e li ragruppo per cluster.
Gli insiemi singoletti (Trieste e Tavagnacco) non sono stati cerchiati.
Si può osservare una buona separazione tra i cluster.

```{r, warning=FALSE, message=FALSE}
# dati originali
data4pca |>
  # dati post rotazione
  bind_cols(
    pca$x |> 
      data.frame()
  ) |>
  # Unisco i cluster di SAS
  left_join(
    read.csv(here::here("data/cluster_top20comuni.csv")),
    by = "Comune"
  ) |> 
  ggplot(aes(PC1, PC2, color = CLUSNAME, fill = CLUSNAME)) +
  geom_point() +
  ggforce::geom_mark_ellipse(
    expand = unit(0.5,"mm"), # riduce gli elissi per una migliore rappresentazione
    alpha = .1,
    size = .5,
    linetype = "dashed"
    ) +
  ggrepel::geom_text_repel(
    aes(label = Comune), # se lo metto in aes() principale poi compare anche negli elissi
    size = 3.5,
    # nudge_x = -1.1,
    segment.size = 0.5,  # Riduce lo spessore della linea
    segment.linetype = "dotted"  # Rende la linea tratteggiata con puntini
            ) +
  theme_minimal() +
  labs(
    x = "Prima Componente Principale",
    y = "Seconda Componente Principale",
    title = "Analisi dei Cluster in una PCA",
    subtitle = "6 cluster ottenuti mediante cluster gerarchica con il metodo di Ward"
  ) +
  theme(legend.position = "none")
```

Osservando il grafico soprastante e il correlogramma con le componenti principali si possono trarre molte conclusioni.
Di seguito alcune con una tabella con i dati per confermare ciò:

1.  Trieste ha la prima componente principale molto negativa, e d'accordo con il correlogramma, ciò è dovuto da un alto valore di indifferenziati, secco e bassi valori, invece, per la raccolta differenziata.

2.  Tavagnacco ha un valore della seconda componente principale estremamente alto. Infatti, non ricicla RAEE, legno e metallo che sono correlati negativamente con la seconda componente principale. Probabilmente è l'unico dei 20 comuni più popolosi del FVG senza un centro di smaltimento dei rifiuti.

3.  Pordenone è il comune più attivo nella raccolta differenziata, infatti ha un tasso di raccolta differenziata del 82,1% confronto il 70,4% della media dei 20 comuni più popolosi del FVG. Mentre, Trieste è il comune con il valore più basso. Motivo per la quale Trieste e Pordenone sono i comuni con la distanza massima della prima componente principale.

4.  Udine è il comune con il valore della seconda componente principale minore. Un'ipotesi potrebbe essere che tutti i comuni più piccoli si rivolgano alla città per lo smaltimento di rifiuti speciali come legno, metalli e RAEE. Per il legno, il valore è quasi il doppio della media.

```{r}
data4pca |> 
  summarise(
    Comune = "Media top 20 FVG",
    across(where(is.numeric), mean),
  ) |> 
  bind_rows(
    data4pca |> 
      filter(Comune %in% c("Trieste", "Pordenone", "Tavagnacco", "Udine"))
  ) |> 
  mutate(
    RD =  scales::percent(RD, accuracy = .1)
  ) |> 
  select(Comune, Indifferenziati, "Tasso Raccolta Differenziata" = RD,
         Legno, Organico) |> 
  print_df()
```
